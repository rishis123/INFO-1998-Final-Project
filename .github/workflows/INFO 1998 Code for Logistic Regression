@Creator: Rishi Shah, Cornell University
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score
from sklearn import preprocessing
from sklearn.metrics import r2_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler


df = pd.read_csv("/Users/rishishah/Downloads/archive/diabetes_012_health_indicators_BRFSS2015.csv")
#print(df) # note: this is the CSV file where Diabetes_012 has 0 for no-diabetes, 1 for pre-diabetes, and 2 for diabetes

#Start w/ Logistic Regression

new_df = df.copy()
new_df.loc[new_df['Diabetes_012'] == 1, 'Diabetes_01'] = 0 #only change if pre-diabetes to no diabetes (for binary)
new_df.loc[new_df['Diabetes_012'] == 2, 'Diabetes_01'] = 2 #keep same if diabetes
new_df.loc[new_df['Diabetes_012'] == 0, 'Diabetes_01'] = 0 #keep same if no diabetes


#print(new_df['Diabetes_012'].tail(50))
#print(new_df['Diabetes_01'].tail(50)) --> code works

new_df = new_df.drop("Diabetes_012", axis='columns') #Diabetes_01 is the new column we want to predict


'''''''''
#Note: Major problem with logistic regression --> there are almost 7 times as many non-diabetics (0) as diabetics (1)
#As such, can just guess non-diabetic (0) for all cases and have around 85% accuracy

Counter0 = 0
Counter1 = 0
for i in new_df['Diabetes_01']:
    if (i == 0):
        Counter0+=1
    else:
        Counter1+=1
print(Counter0) --> 218334
print(Counter1) --> 35346
'''

X = new_df.drop("Diabetes_01", axis='columns') #dataframe of features (don't want Diabetes_01 itself)
Y = new_df['Diabetes_01']

# Scale features to deal with no.of iterations limit error
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)# this yields an array
X = pd.DataFrame(X_scaled, columns=X.columns) #convert back to dataframe


accuracyVal = 0

tot_coeffs = np.zeros(len(X.columns)) #populates array with zeros, equal in length to number of columns in feature dataframe

#Take average of bunch of trials to get average results
for i in range (0,10):
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1998)

    #Create model
    model = LogisticRegression()
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracyVal += accuracy_score(Y_test,Y_pred)
    coefficients = model.coef_[0] #array of each coefficient's weight
    tot_coeffs += coefficients

tot_coeffs = tot_coeffs/10 #divide coefficients over 10 iterations
accuracyVal = accuracyVal/10 #divide accuracy score over 10 iterations
feature_weights = pd.DataFrame({'Feature': X.columns, 'Coefficient': tot_coeffs})
sorted_feature_weights = feature_weights.sort_values(by='Coefficient', ascending=False)

#VISUALIZATION


fig0, ax0 = plt.subplots(figsize=(10, 6))
sorted_feature_weights.plot(kind='barh', ax=ax0, width=0.8)

# Set plot labels and title
ax0.set_xlabel('Feature Weight')
ax0.set_ylabel('Feature Name')
ax0.set_title('Feature Weights of Logistic Regression Model for Diabetes Risk Factors')

#Solution to ticks showing up as numbers corresponding with features rather than the names themselves
ax0.set_yticklabels(sorted_feature_weights['Feature'])

plt.show()
print("Accuracy Score of Logistic Regression:", accuracyVal)



